{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv = '../../ner_dataset.csv'\n",
    "nrows = None\n",
    "input_df = pd.read_csv(input_csv, nrows = nrows,encoding = \"ISO-8859-1\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df['Tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = input_df.drop(columns=['Sentence #', 'POS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_csv('ner_corpus.tsv', sep='\\t', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
    "    try:\n",
    "        f=open(input_path,'r') # input file\n",
    "        fp=open(output_path, 'w') # output file\n",
    "        data_dict={}\n",
    "        annotations =[]\n",
    "        label_dict={}\n",
    "        s=''\n",
    "        start=0\n",
    "        for line in f:\n",
    "            if line[0:len(line)-1]!='.\\tO':\n",
    "                word,entity=line.split('\\t')\n",
    "                s+=word+\" \"\n",
    "                entity=entity[:len(entity)-1]\n",
    "                if entity!=unknown_label:\n",
    "                    if len(entity) != 1:\n",
    "                        d={}\n",
    "                        d['text']=word\n",
    "                        d['start']=start\n",
    "                        d['end']=start+len(word)-1  \n",
    "                        try:\n",
    "                            label_dict[entity].append(d)\n",
    "                        except:\n",
    "                            label_dict[entity]=[]\n",
    "                            label_dict[entity].append(d) \n",
    "                start+=len(word)+1\n",
    "            else:\n",
    "                data_dict['content']=s\n",
    "                s=''\n",
    "                label_list=[]\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text']!=''):\n",
    "                            l=[ents,label_dict[ents][i]]\n",
    "                            for j in range(i+1,len(label_dict[ents])): \n",
    "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
    "                                    di={}\n",
    "                                    di['start']=label_dict[ents][j]['start']\n",
    "                                    di['end']=label_dict[ents][j]['end']\n",
    "                                    di['text']=label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text']=''\n",
    "                            label_list.append(l)                          \n",
    "                            \n",
    "                for entities in label_list:\n",
    "                    label={}\n",
    "                    label['label']=[entities[0]]\n",
    "                    label['points']=entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation']=annotations\n",
    "                annotations=[]\n",
    "                json.dump(data_dict, fp)\n",
    "                fp.write('\\n')\n",
    "                data_dict={}\n",
    "                start=0\n",
    "                label_dict={}\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "tsv_to_json_format(\"ner_corpus.tsv\",'ner_corpus.json','abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"ner_corpus.json\"\n",
    "output_file = \"processed_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "lines=[]\n",
    "with open(input_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    data = json.loads(line)\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    for annotation in data['annotation']:\n",
    "        point = annotation['points'][0]\n",
    "        labels = annotation['label']\n",
    "        if not isinstance(labels, list):\n",
    "            labels = [labels]\n",
    "\n",
    "        for label in labels:\n",
    "            entities.append((point['start'], point['end'] + 1 ,label))\n",
    "\n",
    "\n",
    "    training_data.append((text, {\"entities\" : entities}))\n",
    "\n",
    "# print(training_data)\n",
    "\n",
    "with open(output_file, 'wb') as fp:\n",
    "    pickle.dump(training_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Word Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country ',\n",
       "  {'entities': [(0, 4, 'Tag'),\n",
       "    (53, 59, 'B-geo'),\n",
       "    (82, 86, 'B-geo'),\n",
       "    (116, 123, 'B-gpe')]}),\n",
       " ('Families of soldiers killed in the conflict joined the protesters who carried banners with such slogans as \"\"\"\" Bush Number One Terrorist \"\"\"\" and \"\"\"\" Stop the Bombings ',\n",
       "  {'entities': [(112, 116, 'B-per')]}),\n",
       " ('\"\"\"\" They marched from the Houses of Parliament to a rally in Hyde Park ',\n",
       "  {'entities': [(62, 66, 'B-geo'), (67, 71, 'I-geo')]}),\n",
       " ('Police put the number of marchers at 10,000 while organizers claimed it was 1,00,000 ',\n",
       "  {'entities': []}),\n",
       " (\"The protest comes on the eve of the annual conference of Britain 's ruling Labor Party in the southern English seaside resort of Brighton \",\n",
       "  {'entities': [(57, 64, 'B-geo'),\n",
       "    (129, 137, 'B-geo'),\n",
       "    (75, 80, 'B-org'),\n",
       "    (81, 86, 'I-org'),\n",
       "    (103, 110, 'B-gpe')]})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'ADJ'), ('name', 'NOUN'), ('is', 'VERB'), ('Naitik', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"My name is Naitik\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc[0].ner_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.blank('en')\n",
    "# optimizer = nlp.begin_training()\n",
    "# for i in tqdm(range(20)):\n",
    "#     random.shuffle(training_data)\n",
    "#     for text, annotations in training_data:\n",
    "#         nlp.update([text], [annotations], sgd=optimizer)\n",
    "# # nlp.to_disk(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('London', ''), ('is', ''), ('in', ''), ('England', '')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"London is in England\")\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels\n",
    "for _, annotations in training_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|▌         | 1/20 [09:34<3:02:00, 574.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2268.6041980779205}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 2/20 [20:31<2:59:46, 599.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1988.5744319899613}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 15%|█▌        | 3/20 [31:18<2:53:53, 613.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1982.4172289561357}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 4/20 [41:47<2:44:52, 618.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1957.1333465483972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 5/20 [52:28<2:36:17, 625.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1950.7174705879002}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 30%|███       | 6/20 [1:04:09<2:31:07, 647.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1942.8002620689704}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 35%|███▌      | 7/20 [1:18:44<2:35:09, 716.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1949.5285265756247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 8/20 [1:34:10<2:35:46, 778.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1931.42635524481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 45%|████▌     | 9/20 [1:48:17<2:26:34, 799.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1943.5496148778425}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 10/20 [2:02:38<2:16:19, 817.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1943.8555093970583}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 55%|█████▌    | 11/20 [2:21:43<2:17:23, 915.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1942.6862990617317}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 12/20 [2:36:49<2:01:44, 913.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1920.1488884366577}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 65%|██████▌   | 13/20 [2:50:15<1:42:46, 881.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1929.5339886334423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 70%|███████   | 14/20 [3:06:35<1:31:03, 910.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1929.9855643339454}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 75%|███████▌  | 15/20 [3:26:28<1:22:57, 995.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1906.8587042942052}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 16/20 [3:42:25<1:05:35, 983.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1932.152672894767}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 85%|████████▌ | 17/20 [3:55:00<45:45, 915.13s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1915.788621468646}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 18/20 [4:07:22<28:46, 863.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1936.8314535313098}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 95%|█████████▌| 19/20 [4:19:50<13:48, 828.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1930.882588409388}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 20/20 [4:32:55<00:00, 818.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 1917.192321644872}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes):\n",
    "    for itn in tqdm(range(n_iter)):\n",
    "        random.shuffle(training_data)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                losses=losses,\n",
    "            )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text, _ in TRAIN_DATA:\n",
    "#         doc = nlp(text)\n",
    "#         print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "#         print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_test = spacy.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Gurgaon', 'B-geo')]\n",
      "Tokens [('A', '', 2), ('mansion', '', 2), ('in', '', 2), ('Gurgaon', 'B-geo', 3), ('is', '', 2), ('on', '', 2), ('sale', '', 2)]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_test(\"A mansion in Gurgaon is on sale\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
